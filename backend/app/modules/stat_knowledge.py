"""
Statistical Knowledge Base for Contextual Education.

Provides explanations at different levels (junior/mid/senior) for:
- Statistical terms (p-value, effect size, power, etc.)
- Test selection rationale
- Assumptions and their implications
- Common mistakes
- Academic citations for methodology

Key References for Methodological Rigor:
---------------------------------------
1. de Smith, M. J. (2018). Statistical Analysis Handbook. 
   A comprehensive online reference: https://www.statsref.com/
   
2. Cohen, J. (1988). Statistical Power Analysis for the Behavioral Sciences (2nd ed.).
   Lawrence Erlbaum Associates. [Effect size conventions: d=0.2, 0.5, 0.8]
   
3. Field, A. (2018). Discovering Statistics Using IBM SPSS Statistics (5th ed.).
   SAGE Publications. [Accessible explanations with practical examples]
   
4. Lakens, D. (2013). Calculating and reporting effect sizes to facilitate cumulative 
   science: a practical primer for t-tests and ANOVAs. Frontiers in Psychology, 4, 863.
   https://doi.org/10.3389/fpsyg.2013.00863
   
5. Delacre, M., Lakens, D., & Leys, C. (2017). Why Psychologists Should by Default Use 
   Welch's t-test Instead of Student's t-test. International Review of Social Psychology.
   https://doi.org/10.5334/irsp.82
   
6. Wasserstein, R. L., & Lazar, N. A. (2016). The ASA Statement on p-Values: 
   Context, Process, and Purpose. The American Statistician, 70(2), 129-133.
   https://doi.org/10.1080/00031305.2016.1154108
   
7. American Psychological Association. (2020). Publication Manual of the APA (7th ed.).
   [Reporting standards for statistical results]

8. Faul, F., Erdfelder, E., Lang, A.-G., & Buchner, A. (2007). G*Power 3: 
   A flexible statistical power analysis program. Behavior Research Methods, 39, 175-191.
   [Power analysis methodology]

9. Benjamini, Y., & Hochberg, Y. (1995). Controlling the False Discovery Rate.
   Journal of the Royal Statistical Society B, 57(1), 289-300.
   [FDR correction for multiple comparisons]

10. Tukey, J. W. (1977). Exploratory Data Analysis. Addison-Wesley.
    [Box plots, data visualization principles]

Usage:
    from app.modules.stat_knowledge import get_explanation, get_test_rationale
    
    explanation = get_explanation("p_value", level="junior")
    rationale = get_test_rationale("t_test_ind", data_profile)
    
    # Get citation for academic writing
    citation = get_citation("cohens_d")
"""

from typing import Dict, List, Optional, Any


# =============================================================================
# ACADEMIC REFERENCES (for citation in reports and papers)
# =============================================================================

ACADEMIC_REFERENCES: Dict[str, Dict[str, str]] = {
    "effect_size_conventions": {
        "citation": "Cohen, J. (1988). Statistical Power Analysis for the Behavioral Sciences (2nd ed.). Lawrence Erlbaum Associates.",
        "bibtex": "@book{cohen1988,author={Cohen, Jacob},title={Statistical Power Analysis for the Behavioral Sciences},edition={2nd},publisher={Lawrence Erlbaum Associates},year={1988}}",
        "note": "Conventions for effect size interpretation: d=0.2 (small), d=0.5 (medium), d=0.8 (large)"
    },
    "welch_default": {
        "citation": "Delacre, M., Lakens, D., & Leys, C. (2017). Why Psychologists Should by Default Use Welch's t-test Instead of Student's t-test. International Review of Social Psychology, 30(1), 92-101.",
        "doi": "10.5334/irsp.82",
        "note": "Recommends Welch's t-test as default due to better performance under variance heterogeneity"
    },
    "p_value_statement": {
        "citation": "Wasserstein, R. L., & Lazar, N. A. (2016). The ASA Statement on p-Values: Context, Process, and Purpose. The American Statistician, 70(2), 129-133.",
        "doi": "10.1080/00031305.2016.1154108",
        "note": "Official ASA guidance on p-value interpretation and reporting"
    },
    "fdr_correction": {
        "citation": "Benjamini, Y., & Hochberg, Y. (1995). Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing. Journal of the Royal Statistical Society B, 57(1), 289-300.",
        "note": "FDR procedure for multiple comparison correction"
    },
    "effect_size_primer": {
        "citation": "Lakens, D. (2013). Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and ANOVAs. Frontiers in Psychology, 4, 863.",
        "doi": "10.3389/fpsyg.2013.00863",
        "note": "Practical guide for calculating and reporting effect sizes"
    },
    "power_analysis": {
        "citation": "Faul, F., Erdfelder, E., Lang, A.-G., & Buchner, A. (2007). G*Power 3: A flexible statistical power analysis program. Behavior Research Methods, 39, 175-191.",
        "note": "Reference for power analysis methodology and G*Power software"
    },
    "apa_reporting": {
        "citation": "American Psychological Association. (2020). Publication Manual of the American Psychological Association (7th ed.). APA.",
        "note": "Standard for reporting statistical results in social sciences"
    },
    "de_smith_handbook": {
        "citation": "de Smith, M. J. (2018). Statistical Analysis Handbook. Drumlin Security Ltd.",
        "url": "https://www.statsref.com/",
        "note": "Comprehensive statistical reference with formulas and explanations"
    },
    "field_spss": {
        "citation": "Field, A. (2018). Discovering Statistics Using IBM SPSS Statistics (5th ed.). SAGE Publications.",
        "note": "Comprehensive statistics textbook with accessible explanations"
    },
    "tukey_eda": {
        "citation": "Tukey, J. W. (1977). Exploratory Data Analysis. Addison-Wesley.",
        "note": "Foundational work on data visualization and exploratory analysis"
    },
    "normality_tests": {
        "citation": "Shapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test for normality. Biometrika, 52(3-4), 591-611.",
        "note": "Original Shapiro-Wilk test paper"
    },
    "levene_test": {
        "citation": "Levene, H. (1960). Robust tests for equality of variances. In Contributions to Probability and Statistics (pp. 278-292). Stanford University Press.",
        "note": "Original Levene's test for homogeneity of variances"
    },
    "mann_whitney": {
        "citation": "Mann, H. B., & Whitney, D. R. (1947). On a test of whether one of two random variables is stochastically larger than the other. The Annals of Mathematical Statistics, 18(1), 50-60.",
        "note": "Original Mann-Whitney U test paper"
    },
    "kruskal_wallis": {
        "citation": "Kruskal, W. H., & Wallis, W. A. (1952). Use of ranks in one-criterion variance analysis. Journal of the American Statistical Association, 47(260), 583-621.",
        "note": "Original Kruskal-Wallis test paper"
    },
    "bonferroni": {
        "citation": "Dunn, O. J. (1961). Multiple comparisons among means. Journal of the American Statistical Association, 56(293), 52-64.",
        "note": "Bonferroni correction for multiple comparisons"
    },
    "hedges_g": {
        "citation": "Hedges, L. V. (1981). Distribution theory for Glass's estimator of effect size and related estimators. Journal of Educational Statistics, 6(2), 107-128.",
        "note": "Hedges' g correction for small sample bias in Cohen's d"
    }
}


# =============================================================================
# STATISTICAL TERMS KNOWLEDGE BASE
# =============================================================================

STAT_TERMS: Dict[str, Dict[str, Any]] = {
    
    # -------------------------------------------------------------------------
    # Core Concepts
    # -------------------------------------------------------------------------
    
    "p_value": {
        "term": "P-value",
        "term_ru": "P-–∑–Ω–∞—á–µ–Ω–∏–µ",
        "definition": {
            "junior": "–ß–µ–º –º–µ–Ω—å—à–µ p-value, —Ç–µ–º —Å–∏–ª—å–Ω–µ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –ø—Ä–æ—Ç–∏–≤ –Ω—É–ª–µ–≤–æ–π –≥–∏–ø–æ—Ç–µ–∑—ã. –û–±—ã—á–Ω–æ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç —Å 0.05.",
            "mid": "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É ‚â• –Ω–∞–±–ª—é–¥–∞–µ–º–æ–π –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏, —á—Ç–æ H0 –≤–µ—Ä–Ω–∞. –ó–∞–≤–∏—Å–∏—Ç –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –≤—ã–±–æ—Ä–∫–∏.",
            "senior": "P(data|H0). –ù–µ –ø—É—Ç–∞—Ç—å —Å P(H0|data). –ü—Ä–∏ –±–æ–ª—å—à–æ–º n –¥–∞–∂–µ trivial —ç—Ñ—Ñ–µ–∫—Ç—ã –¥–∞—é—Ç p < 0.05. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –≤–º–µ—Å—Ç–µ —Å effect size –∏ CI."
        },
        "common_mistakes": [
            "p-value ‚â† –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —á—Ç–æ H0 –≤–µ—Ä–Ω–∞",
            "p < 0.05 ‚â† –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å",
            "p > 0.05 ‚â† '—ç—Ñ—Ñ–µ–∫—Ç–∞ –Ω–µ—Ç' (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –º–æ—â–Ω–æ—Å—Ç–∏)"
        ],
        "what_to_check": ["effect_size", "confidence_interval", "power"],
        "emoji": "üìä"
    },
    
    "effect_size": {
        "term": "Effect Size",
        "term_ru": "–†–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞",
        "definition": {
            "junior": "–ù–∞—Å–∫–æ–ª—å–∫–æ –±–æ–ª—å—à–æ–π —ç—Ñ—Ñ–µ–∫—Ç –º—ã –Ω–∞—à–ª–∏. –ù–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –≤—ã–±–æ—Ä–∫–∏.",
            "mid": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ä–∞ —Å–∏–ª—ã —ç—Ñ—Ñ–µ–∫—Ç–∞. Cohen's d = —Ä–∞–∑–Ω–∏—Ü–∞ —Å—Ä–µ–¥–Ω–∏—Ö / pooled SD.",
            "senior": "–ü–æ–∑–≤–æ–ª—è–µ—Ç —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–µ–∂–¥—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º–∏. –î–ª—è –º–µ—Ç–∞–∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–µ–µ p-value."
        },
        "thresholds": {
            "cohens_d": {
                "negligible": {"max": 0.2, "label": "–Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π"},
                "small": {"max": 0.5, "label": "–º–∞–ª—ã–π"},
                "medium": {"max": 0.8, "label": "—Å—Ä–µ–¥–Ω–∏–π"},
                "large": {"min": 0.8, "label": "–±–æ–ª—å—à–æ–π"}
            },
            "eta_squared": {
                "small": {"max": 0.06, "label": "–º–∞–ª—ã–π"},
                "medium": {"max": 0.14, "label": "—Å—Ä–µ–¥–Ω–∏–π"},
                "large": {"min": 0.14, "label": "–±–æ–ª—å—à–æ–π"}
            },
            "partial_eta_squared": {
                "small": {"max": 0.06, "label": "–º–∞–ª—ã–π"},
                "medium": {"max": 0.14, "label": "—Å—Ä–µ–¥–Ω–∏–π"},
                "large": {"min": 0.14, "label": "–±–æ–ª—å—à–æ–π"}
            },
            "r": {
                "weak": {"max": 0.3, "label": "—Å–ª–∞–±–∞—è —Å–≤—è–∑—å"},
                "moderate": {"max": 0.5, "label": "—É–º–µ—Ä–µ–Ω–Ω–∞—è —Å–≤—è–∑—å"},
                "strong": {"min": 0.5, "label": "—Å–∏–ª—å–Ω–∞—è —Å–≤—è–∑—å"}
            }
        },
        "common_mistakes": [
            "–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ effect size –ø—Ä–∏ significant p-value",
            "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ p-value –¥–ª—è –≤—ã–≤–æ–¥–æ–≤"
        ],
        "emoji": "üìè"
    },
    
    "power": {
        "term": "Statistical Power",
        "term_ru": "–ú–æ—â–Ω–æ—Å—Ç—å —Ç–µ—Å—Ç–∞",
        "definition": {
            "junior": "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç, –µ—Å–ª–∏ –æ–Ω —Ä–µ–∞–ª—å–Ω–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è ‚â• 80%.",
            "mid": "Power = 1 - Œ≤, –≥–¥–µ Œ≤ ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—à–∏–±–∫–∏ II —Ä–æ–¥–∞ (–ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç). –ó–∞–≤–∏—Å–∏—Ç –æ—Ç n, effect size, alpha.",
            "senior": "–ü—Ä–∏ power = 0.8 –∏ —Ä–µ–∞–ª—å–Ω–æ–º —ç—Ñ—Ñ–µ–∫—Ç–µ ‚Äî 20% —à–∞–Ω—Å –ø–æ–ª—É—á–∏—Ç—å p > 0.05. Post-hoc power analysis –∏–º–µ–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è."
        },
        "recommendations": {
            "low": {"max": 0.5, "message": "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∑–∫–∞—è –º–æ—â–Ω–æ—Å—Ç—å. –£–≤–µ–ª–∏—á—å—Ç–µ –≤—ã–±–æ—Ä–∫—É."},
            "insufficient": {"max": 0.8, "message": "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –º–æ—â–Ω–æ—Å—Ç—å. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è ‚â• 80%."},
            "adequate": {"max": 0.95, "message": "–ê–¥–µ–∫–≤–∞—Ç–Ω–∞—è –º–æ—â–Ω–æ—Å—Ç—å."},
            "high": {"min": 0.95, "message": "–í—ã—Å–æ–∫–∞—è –º–æ—â–Ω–æ—Å—Ç—å. –í–æ–∑–º–æ–∂–Ω–æ, –≤—ã–±–æ—Ä–∫–∞ –∏–∑–±—ã—Ç–æ—á–Ω–∞."}
        },
        "emoji": "‚ö°"
    },
    
    "alpha": {
        "term": "Alpha Level",
        "term_ru": "–£—Ä–æ–≤–µ–Ω—å –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏",
        "definition": {
            "junior": "–ü–æ—Ä–æ–≥ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è. –û–±—ã—á–Ω–æ 0.05 (5%). –ï—Å–ª–∏ p < alpha ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑–Ω–∞—á–∏–º—ã–π.",
            "mid": "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—à–∏–±–∫–∏ I —Ä–æ–¥–∞ (–ª–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç). –ü—Ä–∏ alpha = 0.05 –≤ 5% —Å–ª—É—á–∞–µ–≤ –æ—Ç–≤–µ—Ä–≥–∞–µ–º –≤–µ—Ä–Ω—É—é H0.",
            "senior": "–ü—Ä–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Ä–∞–≤–Ω–µ–Ω–∏—è—Ö –Ω—É–∂–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏—è (Bonferroni, FDR). –í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö –∏—Å–ø–æ–ª—å–∑—É—é—Ç alpha = 0.005."
        },
        "emoji": "üéØ"
    },
    
    "confidence_interval": {
        "term": "Confidence Interval",
        "term_ru": "–î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª",
        "definition": {
            "junior": "–î–∏–∞–ø–∞–∑–æ–Ω, –≤ –∫–æ—Ç–æ—Ä–æ–º —Å 95% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –∏—Å—Ç–∏–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞.",
            "mid": "–ü—Ä–∏ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ 100 —Ä–∞–∑, ~95 CI –∏–∑ 100 –∑–∞—Ö–≤–∞—Ç—è—Ç –∏—Å—Ç–∏–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.",
            "senior": "CI –¥–ª—è effect size –≤–∞–∂–Ω–µ–µ CI –¥–ª—è mean. –ï—Å–ª–∏ CI –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç 0 ‚Äî —ç—Ñ—Ñ–µ–∫—Ç –∑–Ω–∞—á–∏–º –Ω–∞ –¥–∞–Ω–Ω–æ–º alpha."
        },
        "emoji": "üìê"
    },
    
    # -------------------------------------------------------------------------
    # Assumptions
    # -------------------------------------------------------------------------
    
    "normality": {
        "term": "Normality Assumption",
        "term_ru": "–î–æ–ø—É—â–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏",
        "definition": {
            "junior": "–î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –¥–ª—è t-test –∏ ANOVA.",
            "mid": "–ë–ª–∞–≥–æ–¥–∞—Ä—è –¶–ü–¢, –ø—Ä–∏ n > 30 —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–∏—Ö –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É. –î–ª—è –º–∞–ª—ã—Ö n ‚Äî –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ Shapiro-Wilk.",
            "senior": "T-test —É—Å—Ç–æ–π—á–∏–≤ –∫ –Ω–∞—Ä—É—à–µ–Ω–∏—è–º –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ä–∞–≤–Ω—ã—Ö n –∏ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è—Ö. –ö—Ä–∏—Ç–∏—á–Ω–µ–µ –¥–ª—è –º–∞–ª—ã—Ö –≤—ã–±–æ—Ä–æ–∫."
        },
        "how_to_check": "Shapiro-Wilk test (p > 0.05 ‚Üí –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å), Q-Q plot",
        "if_violated": "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã (Mann-Whitney, Kruskal-Wallis) –∏–ª–∏ bootstrap",
        "emoji": "üìà"
    },
    
    "homogeneity": {
        "term": "Homogeneity of Variance",
        "term_ru": "–ì–æ–º–æ–≥–µ–Ω–Ω–æ—Å—Ç—å –¥–∏—Å–ø–µ—Ä—Å–∏–π",
        "definition": {
            "junior": "–î–∏—Å–ø–µ—Ä—Å–∏–∏ –≤ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º—ã—Ö –≥—Ä—É–ø–ø–∞—Ö –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏.",
            "mid": "Levene's test –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –¥–∏—Å–ø–µ—Ä—Å–∏–π. –ü—Ä–∏ –Ω–∞—Ä—É—à–µ–Ω–∏–∏ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Welch's correction.",
            "senior": "ANOVA —É—Å—Ç–æ–π—á–∏–≤ –∫ –Ω–∞—Ä—É—à–µ–Ω–∏—è–º –ø—Ä–∏ —Ä–∞–≤–Ω—ã—Ö n. –ü—Ä–∏ –Ω–µ—Ä–∞–≤–Ω—ã—Ö n –∏ –≥–µ—Ç–µ—Ä–æ—Å–∫–µ–¥–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ ‚Äî Welch ANOVA –∏–ª–∏ Games-Howell post-hoc."
        },
        "how_to_check": "Levene's test (p > 0.05 ‚Üí –¥–∏—Å–ø–µ—Ä—Å–∏–∏ —Ä–∞–≤–Ω—ã)",
        "if_violated": "Welch's t-test –∏–ª–∏ Welch's ANOVA",
        "emoji": "‚öñÔ∏è"
    },
    
    "independence": {
        "term": "Independence of Observations",
        "term_ru": "–ù–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –Ω–∞–±–ª—é–¥–µ–Ω–∏–π",
        "definition": {
            "junior": "–ö–∞–∂–¥–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –Ω–µ –¥–æ–ª–∂–Ω–æ –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç –¥—Ä—É–≥–∏—Ö.",
            "mid": "–ù–∞—Ä—É—à–∞–µ—Ç—Å—è –ø—Ä–∏ repeated measures, –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–∞—Ö.",
            "senior": "–î–ª—è –∑–∞–≤–∏—Å–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö ‚Äî paired tests, mixed models, GEE. –ü—Ä–∏ —Å–µ—Ç–µ–≤—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–∞—Ö ‚Äî cluster-robust SE."
        },
        "examples_violated": [
            "–ù–µ—Å–∫–æ–ª—å–∫–æ –∏–∑–º–µ—Ä–µ–Ω–∏–π –æ—Ç –æ–¥–Ω–æ–≥–æ –ø–∞—Ü–∏–µ–Ω—Ç–∞",
            "–°—Ç—É–¥–µ–Ω—Ç—ã –∏–∑ –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞",
            "–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã"
        ],
        "emoji": "üîó"
    },
    
    # -------------------------------------------------------------------------
    # Effect Size Types
    # -------------------------------------------------------------------------
    
    "cohens_d": {
        "term": "Cohen's d",
        "term_ru": "d –ö–æ—ç–Ω–∞",
        "definition": {
            "junior": "–†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏ –≤ –µ–¥–∏–Ω–∏—Ü–∞—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è.",
            "mid": "d = (M1 - M2) / SD_pooled. –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: 0.2 –º–∞–ª—ã–π, 0.5 —Å—Ä–µ–¥–Ω–∏–π, 0.8 –±–æ–ª—å—à–æ–π.",
            "senior": "Hedges' g ‚Äî –∫–æ—Ä—Ä–µ–∫—Ü–∏—è –¥–ª—è –º–∞–ª—ã—Ö –≤—ã–±–æ—Ä–æ–∫. Glass's Œî ‚Äî –∫–æ–≥–¥–∞ SD –≥—Ä—É–ø–ø —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ."
        },
        "formula": "d = (M‚ÇÅ - M‚ÇÇ) / SD_pooled",
        "practical_meaning": {
            0.2: "~58% –≥—Ä—É–ø–ø—ã A –≤—ã—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –≥—Ä—É–ø–ø—ã B",
            0.5: "~69% –≥—Ä—É–ø–ø—ã A –≤—ã—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –≥—Ä—É–ø–ø—ã B",
            0.8: "~79% –≥—Ä—É–ø–ø—ã A –≤—ã—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –≥—Ä—É–ø–ø—ã B"
        },
        "emoji": "üìä"
    },
    
    "eta_squared": {
        "term": "Eta-squared (Œ∑¬≤)",
        "term_ru": "–≠—Ç–∞-–∫–≤–∞–¥—Ä–∞—Ç",
        "definition": {
            "junior": "–î–æ–ª—è –¥–∏—Å–ø–µ—Ä—Å–∏–∏, –æ–±—ä—è—Å–Ω—è–µ–º–∞—è —Ñ–∞–∫—Ç–æ—Ä–æ–º. –ê–Ω–∞–ª–æ–≥ R¬≤ –¥–ª—è ANOVA.",
            "mid": "Œ∑¬≤ = SS_between / SS_total. Partial Œ∑¬≤ —É—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ relevant variance.",
            "senior": "Œ∑¬≤ –ø–µ—Ä–µ–æ—Ü–µ–Ω–∏–≤–∞–µ—Ç effect –≤ –≤—ã–±–æ—Ä–∫–µ. œâ¬≤ ‚Äî –º–µ–Ω–µ–µ —Å–º–µ—â—ë–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –ø–æ–ø—É–ª—è—Ü–∏–∏."
        },
        "formula": "Œ∑¬≤ = SS_between / SS_total",
        "emoji": "üìê"
    },

    "multiple_comparison": {
        "term": "Multiple Comparison Correction",
        "term_ru": "–ö–æ—Ä—Ä–µ–∫—Ü–∏—è –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è",
        "definition": {
            "junior": "–ö–æ–≥–¥–∞ –¥–µ–ª–∞–µ—à—å –º–Ω–æ–≥–æ —Ç–µ—Å—Ç–æ–≤, —à–∞–Ω—Å –ª–æ–∂–Ω–æ–π –Ω–∞—Ö–æ–¥–∫–∏ —Ä–∞—Å—Ç—ë—Ç. –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —ç—Ç–æ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç.",
            "mid": "–ü—Ä–∏ 20 —Ç–µ—Å—Ç–∞—Ö —Å Œ±=0.05 –æ–∂–∏–¥–∞–µ—Ç—Å—è 1 –ª–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π. FDR –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –¥–æ–ª—é –ª–æ–∂–Ω—ã—Ö —Å—Ä–µ–¥–∏ –∑–Ω–∞—á–∏–º—ã—Ö.",
            "senior": "FWER vs FDR. Bonferroni: Œ±/n, –æ—á–µ–Ω—å –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–µ–Ω. BH: step-up, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç E[V/R]. BY: –¥–ª—è –∑–∞–≤–∏—Å–∏–º—ã—Ö —Ç–µ—Å—Ç–æ–≤."
        },
        "methods": {
            "bonferroni": {
                "name": "Bonferroni",
                "formula": "Œ±_adj = Œ± / n",
                "description_ru": "–°–∞–º—ã–π —Å—Ç—Ä–æ–≥–∏–π. –î–µ–ª–∏—Ç Œ± –Ω–∞ —á–∏—Å–ª–æ —Ç–µ—Å—Ç–æ–≤.",
                "when_to_use": "–ö–æ–≥–¥–∞ –ª–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º"
            },
            "holm": {
                "name": "Holm-Bonferroni",
                "description_ru": "–ß—É—Ç—å –º—è–≥—á–µ Bonferroni. Step-down –ø—Ä–æ—Ü–µ–¥—É—Ä–∞.",
                "when_to_use": "–ö–æ–≥–¥–∞ Bonferroni —Å–ª–∏—à–∫–æ–º –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–µ–Ω"
            },
            "bh": {
                "name": "Benjamini-Hochberg",
                "description_ru": "FDR –∫–æ–Ω—Ç—Ä–æ–ª—å. –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –¥–æ–ª—é –ª–æ–∂–Ω—ã—Ö –Ω–∞—Ö–æ–¥–æ–∫.",
                "when_to_use": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑, –º–Ω–æ–≥–æ —Ç–µ—Å—Ç–æ–≤"
            },
            "by": {
                "name": "Benjamini-Yekutieli",
                "description_ru": "FDR –¥–ª—è –∑–∞–≤–∏—Å–∏–º—ã—Ö —Ç–µ—Å—Ç–æ–≤.",
                "when_to_use": "–ö–æ–≥–¥–∞ —Ç–µ—Å—Ç—ã –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç –º–µ–∂–¥—É —Å–æ–±–æ–π"
            }
        },
        "recommendation": "–î–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: BH-FDR. –î–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—â–µ–≥–æ: Bonferroni –∏–ª–∏ Holm.",
        "common_mistakes": [
            "–ù–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Ä–∞–≤–Ω–µ–Ω–∏—è—Ö",
            "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Bonferroni –∫–æ–≥–¥–∞ BH –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ",
            "–ü—É—Ç–∞—Ç—å FWER –∏ FDR"
        ],
        "emoji": "üî¢"
    }
}


# =============================================================================
# TEST SELECTION RATIONALE
# =============================================================================

TEST_KNOWLEDGE: Dict[str, Dict[str, Any]] = {
    
    "t_test_ind": {
        "name": "Independent Samples t-test",
        "name_ru": "T-test –¥–ª—è –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –≤—ã–±–æ—Ä–æ–∫",
        "when_to_use": [
            "2 –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –≥—Ä—É–ø–ø—ã",
            "Numeric outcome (–Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)",
            "–ù–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (–∏–ª–∏ n > 30)",
            "–ü—Ä–∏–º–µ—Ä–Ω–æ —Ä–∞–≤–Ω—ã–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏"
        ],
        "assumptions": ["normality", "homogeneity", "independence"],
        "why_it_works": {
            "junior": "–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å—Ä–µ–¥–Ω–∏–µ –¥–≤—É—Ö –≥—Ä—É–ø–ø –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –∑–Ω–∞—á–∏–º–∞ –ª–∏ —Ä–∞–∑–Ω–∏—Ü–∞.",
            "mid": "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç t-—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ. –ü—Ä–∏ n ‚Üí ‚àû –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è –∫ z-test –±–ª–∞–≥–æ–¥–∞—Ä—è –¶–ü–¢.",
            "senior": "Pooled variance estimate –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç œÉ‚ÇÅ = œÉ‚ÇÇ. –ü—Ä–∏ –Ω–∞—Ä—É—à–µ–Ω–∏–∏ ‚Äî Welch's correction —Å Satterthwaite df."
        },
        "alternatives": {
            "non_normal": {"test": "mann_whitney", "reason": "–µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ–Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ"},
            "unequal_variance": {"test": "welch_t_test", "reason": "–µ—Å–ª–∏ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è"},
            "small_n": {"test": "permutation_test", "reason": "–µ—Å–ª–∏ n < 15 –∏ –Ω–µ–Ω–æ—Ä–º–∞–ª—å–Ω–æ"}
        },
        "effect_size": "cohens_d",
        "emoji": "üìä"
    },
    
    "welch_t_test": {
        "name": "Welch's t-test",
        "name_ru": "T-test –£—ç–ª—á–∞",
        "when_to_use": [
            "2 –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –≥—Ä—É–ø–ø—ã",
            "–î–∏—Å–ø–µ—Ä—Å–∏–∏ –º–æ–≥—É—Ç —Ä–∞–∑–ª–∏—á–∞—Ç—å—Å—è",
            "–ë–æ–ª–µ–µ robust —á–µ–º Student's t-test"
        ],
        "assumptions": ["normality", "independence"],
        "why_it_works": {
            "junior": "–ö–∞–∫ –æ–±—ã—á–Ω—ã–π t-test, –Ω–æ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Ä–∞–≤–Ω—ã—Ö –¥–∏—Å–ø–µ—Ä—Å–∏–π.",
            "mid": "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Satterthwaite approximation –¥–ª—è degrees of freedom.",
            "senior": "–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –≤–º–µ—Å—Ç–æ Student's t-test (Delacre et al., 2017)."
        },
        "effect_size": "cohens_d",
        "emoji": "üìä"
    },
    
    "mann_whitney": {
        "name": "Mann-Whitney U test",
        "name_ru": "U-—Ç–µ—Å—Ç –ú–∞–Ω–Ω–∞-–£–∏—Ç–Ω–∏",
        "when_to_use": [
            "2 –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –≥—Ä—É–ø–ø—ã",
            "–ù–µ–Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ",
            "Ordinal –∏–ª–∏ skewed numeric –¥–∞–Ω–Ω—ã–µ"
        ],
        "assumptions": ["independence"],
        "why_it_works": {
            "junior": "–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–∞–Ω–≥–∏ (–ø–æ—Ä—è–¥–æ–∫) –≤–º–µ—Å—Ç–æ —Å—Ä–µ–¥–Ω–∏—Ö. –ù–µ —Ç—Ä–µ–±—É–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏.",
            "mid": "–¢–µ—Å—Ç–∏—Ä—É–µ—Ç H0: P(X > Y) = 0.5. –≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–µ–Ω Wilcoxon rank-sum test.",
            "senior": "–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ —Ä–∞–∑–ª–∏—á–∏—è–º –≤ —Ñ–æ—Ä–º–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π. –ü—Ä–∏ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ö –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è ‚â† '—Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–¥–∏–∞–Ω'."
        },
        "effect_size": "rank_biserial",
        "emoji": "üìä"
    },
    
    "anova": {
        "name": "One-way ANOVA",
        "name_ru": "–û–¥–Ω–æ—Ñ–∞–∫—Ç–æ—Ä–Ω—ã–π –¥–∏—Å–ø–µ—Ä—Å–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑",
        "when_to_use": [
            "3+ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –≥—Ä—É–ø–ø",
            "–ù–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ",
            "–†–∞–≤–Ω—ã–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏"
        ],
        "assumptions": ["normality", "homogeneity", "independence"],
        "why_it_works": {
            "junior": "–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏. –ï—Å–ª–∏ p < 0.05 ‚Äî —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –ø–∞—Ä–∞ —Ä–∞–∑–ª–∏—á–∞–µ—Ç—Å—è.",
            "mid": "F = MS_between / MS_within. –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏—é –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏ —Å –≤–∞—Ä–∏–∞—Ü–∏–µ–π –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø.",
            "senior": "ANOVA = special case of linear regression. Robust –∫ –Ω–∞—Ä—É—à–µ–Ω–∏—è–º –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ä–∞–≤–Ω—ã—Ö n."
        },
        "post_hoc": ["tukey", "bonferroni", "holm"],
        "alternatives": {
            "unequal_variance": {"test": "welch_anova", "reason": "–µ—Å–ª–∏ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è"},
            "non_normal": {"test": "kruskal_wallis", "reason": "–µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ–Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ"}
        },
        "effect_size": "eta_squared",
        "emoji": "üìà"
    },
    
    "kruskal_wallis": {
        "name": "Kruskal-Wallis H test",
        "name_ru": "H-—Ç–µ—Å—Ç –ö—Ä–∞—Å–∫–µ–ª–∞-–£–æ–ª–ª–∏—Å–∞",
        "when_to_use": [
            "3+ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –≥—Ä—É–ø–ø",
            "–ù–µ–Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ",
            "Ordinal –∏–ª–∏ skewed –¥–∞–Ω–Ω—ã–µ"
        ],
        "assumptions": ["independence"],
        "why_it_works": {
            "junior": "–ù–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–æ–≥ ANOVA. –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–∞–Ω–≥–∏ –≤–º–µ—Å—Ç–æ —Å—Ä–µ–¥–Ω–∏—Ö.",
            "mid": "H-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —Å—É–º–º–µ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ —Ä–∞–Ω–≥–æ–≤.",
            "senior": "Post-hoc: Dunn's test —Å –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."
        },
        "effect_size": "epsilon_squared",
        "emoji": "üìà"
    },
    
    "chi_square": {
        "name": "Chi-squared test",
        "name_ru": "–•–∏-–∫–≤–∞–¥—Ä–∞—Ç —Ç–µ—Å—Ç",
        "when_to_use": [
            "–î–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ",
            "–¢–∞–±–ª–∏—Ü–∞ —á–∞—Å—Ç–æ—Ç",
            "Expected count ‚â• 5 –≤ –∫–∞–∂–¥–æ–π —è—á–µ–π–∫–µ"
        ],
        "assumptions": ["independence", "expected_count_>=5"],
        "why_it_works": {
            "junior": "–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–≤—è–∑—å –º–µ–∂–¥—É –¥–≤—É–º—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏.",
            "mid": "œá¬≤ = Œ£(O - E)¬≤ / E. –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –Ω–∞–±–ª—é–¥–∞–µ–º—ã–µ —á–∞—Å—Ç–æ—Ç—ã —Å –æ–∂–∏–¥–∞–µ–º—ã–º–∏ –ø—Ä–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏.",
            "senior": "–ü—Ä–∏ 2√ó2 ‚Äî Yates correction –∏–ª–∏ Fisher's exact. –ü—Ä–∏ large samples ‚Äî œá¬≤ robust."
        },
        "alternatives": {
            "small_sample": {"test": "fisher_exact", "reason": "–µ—Å–ª–∏ expected count < 5"}
        },
        "effect_size": "cramers_v",
        "emoji": "üìä"
    },
    
    "pearson": {
        "name": "Pearson correlation",
        "name_ru": "–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ü–∏—Ä—Å–æ–Ω–∞",
        "when_to_use": [
            "–î–≤–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ",
            "–õ–∏–Ω–µ–π–Ω–∞—è —Å–≤—è–∑—å",
            "Bivariate normality"
        ],
        "assumptions": ["normality", "linearity", "homoscedasticity"],
        "why_it_works": {
            "junior": "–ò–∑–º–µ—Ä—è–µ—Ç —Å–∏–ª—É –ª–∏–Ω–µ–π–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç -1 –¥–æ +1.",
            "mid": "r = cov(X,Y) / (SD_X √ó SD_Y). –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ outliers.",
            "senior": "r¬≤ = –¥–æ–ª—è –æ–±—ä—è—Å–Ω—ë–Ω–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏. –ù–µ —É–ª–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ —Å–≤—è–∑–∏."
        },
        "alternatives": {
            "non_linear": {"test": "spearman", "reason": "–¥–ª—è –º–æ–Ω–æ—Ç–æ–Ω–Ω—ã—Ö –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö —Å–≤—è–∑–µ–π"},
            "outliers": {"test": "spearman", "reason": "–±–æ–ª–µ–µ robust –∫ –≤—ã–±—Ä–æ—Å–∞–º"}
        },
        "effect_size": "r",
        "emoji": "üìà"
    },
    
    "spearman": {
        "name": "Spearman correlation",
        "name_ru": "–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –°–ø–∏—Ä–º–µ–Ω–∞",
        "when_to_use": [
            "Ordinal –¥–∞–Ω–Ω—ã–µ",
            "–ù–µ–Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ",
            "–ú–æ–Ω–æ—Ç–æ–Ω–Ω–∞—è (–Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ª–∏–Ω–µ–π–Ω–∞—è) —Å–≤—è–∑—å"
        ],
        "assumptions": ["monotonic_relationship"],
        "why_it_works": {
            "junior": "–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø–æ —Ä–∞–Ω–≥–∞–º. –ë–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤ –∫ –≤—ã–±—Ä–æ—Å–∞–º.",
            "mid": "œÅ = Pearson r –¥–ª—è —Ä–∞–Ω–≥–æ–≤. –£–ª–∞–≤–ª–∏–≤–∞–µ—Ç –º–æ–Ω–æ—Ç–æ–Ω–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ —Å–≤—è–∑–∏.",
            "senior": "–ü—Ä–∏ tied ranks ‚Äî –∫–æ—Ä—Ä–µ–∫—Ü–∏—è. –î–ª—è ordinal –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ Pearson."
        },
        "effect_size": "rho",
        "emoji": "üìà"
    }
}


# =============================================================================
# API FUNCTIONS
# =============================================================================

def get_explanation(term: str, level: str = "junior") -> Optional[Dict[str, Any]]:
    """
    Get explanation for a statistical term at specified level.
    
    Args:
        term: Term key (e.g., "p_value", "effect_size")
        level: "junior", "mid", or "senior"
    
    Returns:
        Dictionary with term, definition, common_mistakes, etc.
    """
    if term not in STAT_TERMS:
        return None
    
    knowledge = STAT_TERMS[term]
    definition = knowledge.get("definition", {})
    
    return {
        "term": knowledge.get("term", term),
        "term_ru": knowledge.get("term_ru", term),
        "definition": definition.get(level, definition.get("junior", "")),
        "common_mistakes": knowledge.get("common_mistakes", []),
        "what_to_check": knowledge.get("what_to_check", []),
        "emoji": knowledge.get("emoji", "üìä")
    }


def get_test_rationale(
    test_id: str, 
    data_profile: Optional[Dict[str, Any]] = None,
    level: str = "junior"
) -> Optional[Dict[str, Any]]:
    """
    Get rationale for why a test was chosen.
    
    Args:
        test_id: Test identifier (e.g., "t_test_ind", "anova")
        data_profile: Data characteristics (n_groups, normality checks, etc.)
        level: Explanation depth
    
    Returns:
        Dictionary with test info, rationale, assumptions, alternatives
    """
    if test_id not in TEST_KNOWLEDGE:
        return None
    
    knowledge = TEST_KNOWLEDGE[test_id]
    why = knowledge.get("why_it_works", {})
    
    result = {
        "test_id": test_id,
        "name": knowledge.get("name", test_id),
        "name_ru": knowledge.get("name_ru", test_id),
        "when_to_use": knowledge.get("when_to_use", []),
        "why_it_works": why.get(level, why.get("junior", "")),
        "assumptions": knowledge.get("assumptions", []),
        "alternatives": knowledge.get("alternatives", {}),
        "effect_size": knowledge.get("effect_size"),
        "emoji": knowledge.get("emoji", "üìä")
    }
    
    # Add assumption checks if data_profile provided
    if data_profile:
        result["assumption_checks"] = _check_assumptions(
            knowledge.get("assumptions", []),
            data_profile
        )
    
    return result


def get_effect_size_interpretation(
    effect_type: str, 
    value: float
) -> Dict[str, Any]:
    """
    Get interpretation of effect size value.
    
    Args:
        effect_type: "cohens_d", "eta_squared", "r", etc.
        value: Numeric effect size value
    
    Returns:
        Dictionary with interpretation, label, percentile info
    """
    if "effect_size" not in STAT_TERMS:
        return {"label": "unknown", "interpretation": ""}
    
    thresholds = STAT_TERMS["effect_size"].get("thresholds", {})
    
    if effect_type == "eta_squared":
        effect_type = "partial_eta_squared" if "partial_eta_squared" in thresholds else effect_type

    if effect_type not in thresholds:
        return {"label": "unknown", "interpretation": ""}
    
    type_thresholds = thresholds[effect_type]
    abs_value = abs(value)
    
    # Find the appropriate category
    label = "unknown"
    for category, bounds in type_thresholds.items():
        max_val = bounds.get("max", float("inf"))
        min_val = bounds.get("min", 0)
        
        if min_val <= abs_value <= max_val:
            label = bounds.get("label", category)
            break
        elif "min" in bounds and abs_value >= min_val:
            label = bounds.get("label", category)
            break
    
    # Practical meaning for Cohen's d
    practical = ""
    if effect_type == "cohens_d":
        cohens_practical = STAT_TERMS.get("cohens_d", {}).get("practical_meaning", {})
        closest = min(cohens_practical.keys(), key=lambda x: abs(x - abs_value), default=None)
        if closest:
            practical = cohens_practical[closest]
    
    return {
        "value": value,
        "abs_value": abs_value,
        "type": effect_type,
        "label": label,
        "label_ru": label,  # Already in Russian from thresholds
        "practical_meaning": practical,
        "direction": "positive" if value > 0 else "negative" if value < 0 else "none"
    }


def get_power_recommendation(power: float) -> Dict[str, Any]:
    """
    Get recommendation based on power value.
    
    Args:
        power: Statistical power (0-1)
    
    Returns:
        Dictionary with status, message, recommendation
    """
    if power < 0.5:
        return {
            "status": "critical",
            "status_ru": "–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∑–∫–∞—è",
            "message": "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∑–∫–∞—è –º–æ—â–Ω–æ—Å—Ç—å. –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç.",
            "recommendation": "–£–≤–µ–ª–∏—á—å—Ç–µ —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ.",
            "icon": "üî¥"
        }
    elif power < 0.8:
        return {
            "status": "insufficient",
            "status_ru": "–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è",
            "message": f"–ú–æ—â–Ω–æ—Å—Ç—å {power:.0%} –Ω–∏–∂–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö 80%.",
            "recommendation": "–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω—ã—Ö –≤—ã–≤–æ–¥–æ–≤.",
            "icon": "üü°"
        }
    elif power < 0.95:
        return {
            "status": "adequate",
            "status_ru": "–∞–¥–µ–∫–≤–∞—Ç–Ω–∞—è",
            "message": f"–ú–æ—â–Ω–æ—Å—Ç—å {power:.0%} ‚Äî –∞–¥–µ–∫–≤–∞—Ç–Ω–∞ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∞.",
            "recommendation": None,
            "icon": "üü¢"
        }
    else:
        return {
            "status": "high",
            "status_ru": "–≤—ã—Å–æ–∫–∞—è",
            "message": f"–ú–æ—â–Ω–æ—Å—Ç—å {power:.0%} ‚Äî –≤—ã—Å–æ–∫–∞—è.",
            "recommendation": "–í—ã–±–æ—Ä–∫–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–∑–±—ã—Ç–æ—á–Ω–æ–π –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ effect size.",
            "icon": "üü¢"
        }


def _check_assumptions(
    assumptions: List[str], 
    data_profile: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """Check assumptions against data profile."""
    results = []
    
    for assumption in assumptions:
        check = {
            "assumption": assumption,
            "term": STAT_TERMS.get(assumption, {}).get("term", assumption),
            "passed": None,
            "p_value": None,
            "note": ""
        }
        
        if assumption == "normality":
            shapiro_p = data_profile.get("shapiro_p")
            if shapiro_p is not None:
                check["passed"] = shapiro_p > 0.05
                check["p_value"] = shapiro_p
                check["note"] = "Shapiro-Wilk test"
                if not check["passed"]:
                    check["recommendation"] = STAT_TERMS.get("normality", {}).get("if_violated", "")
        
        elif assumption == "homogeneity":
            levene_p = data_profile.get("levene_p")
            if levene_p is not None:
                check["passed"] = levene_p > 0.05
                check["p_value"] = levene_p
                check["note"] = "Levene's test"
                if not check["passed"]:
                    check["recommendation"] = STAT_TERMS.get("homogeneity", {}).get("if_violated", "")
        
        elif assumption == "independence":
            check["passed"] = data_profile.get("independence", True)
            check["note"] = "Assumed based on study design"
        
        results.append(check)
    
    return results


def get_all_terms() -> List[Dict[str, str]]:
    """Get list of all available statistical terms."""
    return [
        {
            "key": key,
            "term": val.get("term", key),
            "term_ru": val.get("term_ru", key),
            "emoji": val.get("emoji", "üìä")
        }
        for key, val in STAT_TERMS.items()
    ]


def get_all_tests() -> List[Dict[str, str]]:
    """Get list of all available statistical tests with info."""
    return [
        {
            "key": key,
            "name": val.get("name", key),
            "name_ru": val.get("name_ru", key),
            "emoji": val.get("emoji", "üìä")
        }
        for key, val in TEST_KNOWLEDGE.items()
    ]


def get_citation(reference_key: str) -> Optional[Dict[str, str]]:
    """
    Get academic citation for a statistical concept.
    
    Args:
        reference_key: Key from ACADEMIC_REFERENCES (e.g., "effect_size_conventions")
    
    Returns:
        Dictionary with citation, doi, bibtex, note
    
    Example:
        >>> get_citation("effect_size_conventions")
        {"citation": "Cohen, J. (1988)...", "bibtex": "@book{cohen1988,...}"}
    """
    return ACADEMIC_REFERENCES.get(reference_key)


def get_all_references() -> Dict[str, Dict[str, str]]:
    """
    Get all academic references for methodology documentation.
    
    Useful for:
    - Generating bibliography
    - Adding citations to reports
    - Justifying methodological choices
    
    Returns:
        Dictionary of all academic references
    """
    return ACADEMIC_REFERENCES


def get_references_for_test(test_id: str) -> List[Dict[str, str]]:
    """
    Get relevant references for a specific statistical test.
    
    Args:
        test_id: Test identifier (e.g., "t_test_ind", "mann_whitney")
    
    Returns:
        List of relevant citations
    """
    test_to_refs = {
        "t_test_ind": ["effect_size_conventions", "welch_default", "effect_size_primer"],
        "welch_t_test": ["welch_default", "effect_size_conventions"],
        "mann_whitney": ["mann_whitney", "effect_size_primer"],
        "anova": ["effect_size_conventions", "bonferroni", "effect_size_primer"],
        "kruskal_wallis": ["kruskal_wallis", "bonferroni"],
        "chi_square": ["de_smith_handbook", "field_spss"],
        "pearson": ["de_smith_handbook", "field_spss"],
        "spearman": ["de_smith_handbook", "field_spss"]
    }
    
    ref_keys = test_to_refs.get(test_id, ["de_smith_handbook"])
    return [
        {"key": key, **ACADEMIC_REFERENCES[key]}
        for key in ref_keys
        if key in ACADEMIC_REFERENCES
    ]


def get_reporting_template(test_id: str, result: Dict[str, Any]) -> str:
    """
    Generate APA-style reporting template for statistical result.
    
    Args:
        test_id: Test identifier
        result: Dictionary with p_value, effect_size, etc.
    
    Returns:
        APA-formatted result string (Russian)
    
    Example:
        >>> get_reporting_template("t_test_ind", {"t": 2.45, "df": 48, "p_value": 0.018, "effect_size": 0.71})
        "t(48) = 2.45, p = .018, d = 0.71 [—Å—Ä–µ–¥–Ω–∏–π —ç—Ñ—Ñ–µ–∫—Ç]"
    """
    templates = {
        "t_test_ind": "t({df}) = {stat:.2f}, p {p_str}, d = {effect:.2f} [{effect_label}]",
        "welch_t_test": "t({df:.1f}) = {stat:.2f}, p {p_str}, d = {effect:.2f} [{effect_label}]",
        "mann_whitney": "U = {stat:.0f}, p {p_str}, r = {effect:.2f}",
        "anova": "F({df_between}, {df_within}) = {stat:.2f}, p {p_str}, Œ∑¬≤ = {effect:.3f} [{effect_label}]",
        "kruskal_wallis": "H({df}) = {stat:.2f}, p {p_str}",
        "chi_square": "œá¬≤({df}) = {stat:.2f}, p {p_str}, V = {effect:.2f}",
        "pearson": "r({df}) = {stat:.2f}, p {p_str}",
        "spearman": "œÅ({df}) = {stat:.2f}, p {p_str}"
    }
    
    if test_id not in templates:
        return ""
    
    # Format p-value
    p = result.get("p_value", 1.0)
    if p < 0.001:
        p_str = "< .001"
    else:
        p_str = f"= {p:.3f}".lstrip("0")
    
    # Get effect size label
    effect = result.get("effect_size", 0)
    effect_type = result.get("effect_size_type", "cohens_d")
    effect_info = get_effect_size_interpretation(effect_type, effect)
    effect_label = effect_info.get("label_ru", "")
    
    try:
        return templates[test_id].format(
            stat=result.get("stat_value", result.get("t", result.get("statistic", 0))),
            df=result.get("df", result.get("dof", 0)),
            df_between=result.get("df_between", 0),
            df_within=result.get("df_within", 0),
            p_str=p_str,
            effect=abs(effect),
            effect_label=effect_label
        )
    except (KeyError, ValueError):
        return ""


# =============================================================================
# RECOMMENDED READING BY TOPIC
# =============================================================================

RECOMMENDED_READING = {
    "effect_sizes": [
        "Cohen, J. (1988). Statistical Power Analysis ‚Äî –∫–ª–∞—Å—Å–∏–∫–∞, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è d, r, f",
        "Lakens, D. (2013). Frontiers in Psychology ‚Äî –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ç—É—Ç–æ—Ä–∏–∞–ª —Å —Ñ–æ—Ä–º—É–ª–∞–º–∏",
        "Field, A. (2018). Discovering Statistics ‚Äî –¥–æ—Å—Ç—É–ø–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ"
    ],
    "p_values": [
        "ASA Statement (Wasserstein & Lazar, 2016) ‚Äî –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –ø–æ p-value",
        "Greenland et al. (2016). European Journal of Epidemiology ‚Äî 25 –º–∏—Ñ–æ–≤ –æ p-value"
    ],
    "power_analysis": [
        "G*Power manual (Faul et al., 2007) ‚Äî –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è —Ä–∞—Å—á—ë—Ç–∞ –º–æ—â–Ω–æ—Å—Ç–∏",
        "Cohen, J. (1992). Psychological Bulletin ‚Äî 'A Power Primer'"
    ],
    "multiple_comparisons": [
        "Benjamini & Hochberg (1995) ‚Äî FDR –∫–æ—Ä—Ä–µ–∫—Ü–∏—è",
        "Dunn (1961) ‚Äî Bonferroni –∫–æ—Ä—Ä–µ–∫—Ü–∏—è"
    ],
    "nonparametric": [
        "Mann & Whitney (1947) ‚Äî –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è U-—Ç–µ—Å—Ç–∞",
        "Kruskal & Wallis (1952) ‚Äî H-—Ç–µ—Å—Ç –¥–ª—è 3+ –≥—Ä—É–ø–ø"
    ],
    "general_reference": [
        "de Smith, M. J. (2018). statsref.com ‚Äî –æ–Ω–ª–∞–π–Ω —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫",
        "APA Publication Manual (7th ed.) ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –æ—Ç—á—ë—Ç–Ω–æ—Å—Ç–∏"
    ]
}


def get_recommended_reading(topic: str = "general_reference") -> List[str]:
    """
    Get recommended reading list for a topic.
    
    Args:
        topic: "effect_sizes", "p_values", "power_analysis", 
               "multiple_comparisons", "nonparametric", "general_reference"
    
    Returns:
        List of recommended sources
    """
    return RECOMMENDED_READING.get(topic, RECOMMENDED_READING["general_reference"])
