# üî¨ SCIENTIFIC_STANDARDS.md ‚Äî# Scientific Standards for StatWizard

> –°—Ç–∞–Ω–¥–∞—Ä—Ç—ã –Ω–∞—É—á–Ω–æ–≥–æ –∫–æ–¥–∞ –¥–ª—è AI-–∞–≥–µ–Ω—Ç–æ–≤ –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤.  
> –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞: Python Data Science Handbook, Nathan Yau "Visualize This", de Smith "Statistical Analysis Handbook"

## üìö –û—Å–Ω–æ–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏

1. **Python Data Science Handbook** (Jake VanderPlas) ‚Äî NumPy, Pandas, Matplotlib best practices
2. **Visualize This** (Nathan Yau) ‚Äî FlowingData –ø—Ä–∏–Ω—Ü–∏–ø—ã –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
3. **Statistical Analysis Handbook** (Dr. Michael J. de Smith) ‚Äî –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä–æ–≥–æ—Å—Ç—å
   - Online: <https://www.statsref.com/>
4. **Cohen (1988)** ‚Äî Effect size conventions (d = 0.2, 0.5, 0.8)
5. **APA Publication Manual (7th ed.)** ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –æ—Ç—á—ë—Ç–Ω–æ—Å—Ç–∏
6. **ASA Statement on p-Values (2016)** ‚Äî –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è p-value
7. Pandas Best Practices 2024
8. SciPy / Statsmodels documentation
9. Matplotlib / Seaborn publication standards
10. **Nathan Yau "Visualize This" (FlowingData)** ‚Äî —Å–º. `docs/nathan-yau-visualize-this...pdf`
11. **VISUALIZATION_STYLE_GUIDE.md** ‚Äî –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –≥—Ä–∞—Ñ–∏–∫–∞–º

---

## üìö –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞

1. [NumPy - –û—Å–Ω–æ–≤–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π](#1-numpy)
2. [Pandas - –†–∞–±–æ—Ç–∞ —Å –¥–∞–Ω–Ω—ã–º–∏](#2-pandas)
3. [Matplotlib + Seaborn - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è](#3-visualization)
4. [SciPy + Statsmodels - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞](#4-statistics)
5. [Scikit-learn - ML –∏ –∏–º–ø—É—Ç–∞—Ü–∏—è](#5-machine-learning)
6. [Pingouin - –ë–∏–æ–º–µ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞](#6-pingouin)

---

## 1. NumPy ‚Äî –û—Å–Ω–æ–≤–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π {#1-numpy}

### –ü—Ä–∏–Ω—Ü–∏–ø—ã

| –ü—Ä–∏–Ω—Ü–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|---------|----------|
| **Vectorization** | –ò–∑–±–µ–≥–∞–π —Ü–∏–∫–ª–æ–≤ Python, –∏—Å–ø–æ–ª—å–∑—É–π NumPy –æ–ø–µ—Ä–∞—Ü–∏–∏ |
| **Broadcasting** | –û–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥ –º–∞—Å—Å–∏–≤–∞–º–∏ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ |
| **Memory efficiency** | –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ dtype (float32 –≤–º–µ—Å—Ç–æ float64) |

### –ö–æ–¥-–ø–∞—Ç—Ç–µ—Ä–Ω—ã

```python
import numpy as np

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û ‚Äî Vectorization
result = np.mean(data) * np.std(data)

# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û ‚Äî Python —Ü–∏–∫–ª
total = 0
for x in data:
    total += x
mean = total / len(data)

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û ‚Äî Broadcasting
normalized = (data - data.mean()) / data.std()

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û ‚Äî Boolean indexing
valid_data = data[~np.isnan(data)]
outliers = data[np.abs(data - data.mean()) > 3 * data.std()]
```

---

## 2. Pandas ‚Äî –†–∞–±–æ—Ç–∞ —Å –¥–∞–Ω–Ω—ã–º–∏ {#2-pandas}

### 2.1 –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö

```python
import pandas as pd

# ‚úÖ –õ–£–ß–®–ò–ô —Ñ–æ—Ä–º–∞—Ç ‚Äî Parquet (–≤ 5-10x –±—ã—Å—Ç—Ä–µ–µ CSV)
df = pd.read_parquet("data.parquet", engine="pyarrow")
df.to_parquet("output.parquet", engine="pyarrow")

# CSV ‚Äî –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
df = pd.read_csv("data.csv", 
    dtype={'group': 'category'},  # –£–∫–∞–∑—ã–≤–∞–π —Ç–∏–ø—ã —è–≤–Ω–æ
    parse_dates=['date'],
    usecols=['id', 'value', 'group'],  # –¢–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    na_values=['NA', 'N/A', '-']  # –Ø–≤–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã –ø—Ä–æ–ø—É—Å–∫–æ–≤
)
```

### 2.2 –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏

```python
def optimize_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏.
    –≠–∫–æ–Ω–æ–º–∏—è: 50-75% –ø–∞–º—è—Ç–∏.
    """
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type == 'int64':
            c_min, c_max = df[col].min(), df[col].max()
            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                df[col] = df[col].astype(np.int8)
            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                df[col] = df[col].astype(np.int16)
            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                df[col] = df[col].astype(np.int32)
                
        elif col_type == 'float64':
            df[col] = df[col].astype(np.float32)
            
        elif col_type == 'object':
            if df[col].nunique() / len(df) < 0.5:  # <50% —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö
                df[col] = df[col].astype('category')
    
    return df
```

### 2.3 Method Chaining

```python
# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û ‚Äî –ß–∏—Ç–∞–µ–º—ã–π pipeline
result = (
    df
    .query("age >= 18")
    .assign(bmi=lambda x: x['weight'] / (x['height'] ** 2))
    .pipe(optimize_dataframe)
    .groupby('treatment')
    .agg({'outcome': ['mean', 'std', 'count']})
)

# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û ‚Äî –ö—É—á–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
df1 = df[df['age'] >= 18]
df1['bmi'] = df1['weight'] / (df1['height'] ** 2)
df2 = optimize_dataframe(df1)
result = df2.groupby('treatment').agg(...)
```

### 2.4 –ò–∑–±–µ–≥–∞–π –∏—Ç–µ—Ä–∞—Ü–∏–∏

```python
# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û ‚Äî Vectorized
df['result'] = np.where(df['value'] > 0, 'positive', 'negative')

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û ‚Äî Apply —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–∞ —Å–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞
df['complex_result'] = df['text'].str.extract(r'(\d+)')

# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û ‚Äî iterrows (–º–µ–¥–ª–µ–Ω–Ω–æ!)
for idx, row in df.iterrows():
    df.at[idx, 'result'] = 'positive' if row['value'] > 0 else 'negative'
```

---

## 3. Matplotlib + Seaborn ‚Äî –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è {#3-visualization}

### 3.1 –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–π

```python
import matplotlib.pyplot as plt
import seaborn as sns

# –°—Ç–∏–ª—å –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π
FIGURE_CONFIG = {
    # –†–∞–∑–º–µ—Ä—ã
    'figure.figsize': (7, 5),           # –î—é–π–º—ã
    'figure.dpi': 300,                   # –í—ã—Å–æ–∫–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ
    
    # –®—Ä–∏—Ñ—Ç—ã
    'font.family': 'sans-serif',
    'font.sans-serif': ['Arial', 'Helvetica'],
    'font.size': 10,
    'axes.titlesize': 12,
    'axes.labelsize': 10,
    'xtick.labelsize': 9,
    'ytick.labelsize': 9,
    'legend.fontsize': 9,
    
    # –õ–∏–Ω–∏–∏
    'axes.linewidth': 1.0,
    'lines.linewidth': 1.5,
    'lines.markersize': 6,
    
    # –°–µ—Ç–∫–∞
    'axes.grid': True,
    'grid.alpha': 0.3,
    
    # –£–±—Ä–∞—Ç—å –ª–∏—à–Ω–µ–µ
    'axes.spines.top': False,
    'axes.spines.right': False,
}

plt.rcParams.update(FIGURE_CONFIG)
sns.set_theme(style="whitegrid", palette="colorblind")
```

### 3.2 –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã —Ñ–∏–≥—É—Ä

| –¢–∏–ø | –®–∏—Ä–∏–Ω–∞ | –í—ã—Å–æ—Ç–∞ | –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ |
|-----|--------|--------|---------------|
| Single column | 3.25" | 2.5" | –ñ—É—Ä–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è (1 –∫–æ–ª–æ–Ω–∫–∞) |
| Double column | 7.0" | 5.0" | –ñ—É—Ä–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è (2 –∫–æ–ª–æ–Ω–∫–∏) |
| Presentation | 10" | 6" | –°–ª–∞–π–¥—ã |
| Dashboard | Auto | Auto | –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å |

### 3.3 –¶–≤–µ—Ç–æ–≤—ã–µ –ø–∞–ª–∏—Ç—Ä—ã

```python
# ‚úÖ Perceptually uniform –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
cmap_continuous = 'viridis'  # –∏–ª–∏ 'plasma', 'magma'

# ‚úÖ Diverging –¥–ª—è –¥–∞–Ω–Ω—ã—Ö —Å —Ü–µ–Ω—Ç—Ä–æ–º (z-scores)
cmap_diverging = 'RdBu_r'  # –∏–ª–∏ 'coolwarm'

# ‚úÖ Categorical –¥–ª—è –≥—Ä—É–ø–ø (colorblind-safe)
palette_categorical = 'colorblind'  # seaborn
# –ò–ª–∏ —è–≤–Ω–æ: ['#0173B2', '#DE8F05', '#029E73', '#D55E00', '#CC78BC']

# ‚ùå –ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
# 'jet', 'rainbow' ‚Äî –Ω–µ perceptually uniform
# red/green –≤–º–µ—Å—Ç–µ ‚Äî –ø—Ä–æ–±–ª–µ–º—ã –¥–ª—è colorblind
```

### 3.4 –¢–∏–ø—ã –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø–æ –∞–Ω–∞–ª–∏–∑—É

| –ê–Ω–∞–ª–∏–∑ | –û—Å–Ω–æ–≤–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫ | –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π |
|--------|-----------------|----------------|
| –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥—Ä—É–ø–ø (2) | Box + Strip plot | Violin plot |
| –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥—Ä—É–ø–ø (3+) | Box plot | Bar + Error bars |
| –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è | Scatter + —Ä–µ–≥—Ä–µ—Å—Å–∏—è | Heatmap –º–∞—Ç—Ä–∏—Ü–∞ |
| –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ | Histogram + KDE | QQ-plot |
| –í—Ä–µ–º—è | Line plot + CI | Area plot |
| –í—ã–∂–∏–≤–∞–µ–º–æ—Å—Ç—å | Kaplan-Meier | Forest plot |
| ROC | ROC curve | Precision-Recall |

### 3.5 –®–∞–±–ª–æ–Ω—ã –≥—Ä–∞—Ñ–∏–∫–æ–≤

```python
def plot_group_comparison(df, x, y, ax=None):
    """
    –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≥—Ä—É–ø–ø.
    Box + Strip + –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏.
    """
    if ax is None:
        fig, ax = plt.subplots(figsize=(4, 4))
    
    # Box plot (–±–µ–∑ –≤—ã–±—Ä–æ—Å–æ–≤)
    sns.boxplot(data=df, x=x, y=y, ax=ax, 
                showfliers=False, width=0.5, 
                boxprops={'facecolor': 'lightblue', 'alpha': 0.7})
    
    # –¢–æ—á–∫–∏
    sns.stripplot(data=df, x=x, y=y, ax=ax,
                  color='darkblue', alpha=0.6, size=4, jitter=True)
    
    # –£–±—Ä–∞—Ç—å –ª–∏—à–Ω–∏–µ –æ—Å–∏
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    
    return ax

def plot_correlation_matrix(corr_matrix, ax=None):
    """
    –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ç–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π.
    """
    if ax is None:
        fig, ax = plt.subplots(figsize=(8, 6))
    
    # –ú–∞—Å–∫–∞ –¥–ª—è –≤–µ—Ä—Ö–Ω–µ–≥–æ —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–∞
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
    
    sns.heatmap(corr_matrix, mask=mask, ax=ax,
                cmap='RdBu_r', center=0,
                vmin=-1, vmax=1,
                annot=True, fmt='.2f',
                square=True, linewidths=0.5)
    
    return ax

def plot_distribution(data, ax=None, title=None):
    """
    –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–æ–π –∏ KDE.
    """
    if ax is None:
        fig, ax = plt.subplots(figsize=(5, 4))
    
    sns.histplot(data, kde=True, ax=ax, 
                 color='steelblue', edgecolor='white')
    
    # –î–æ–±–∞–≤–∏—Ç—å —Å—Ä–µ–¥–Ω—é—é –ª–∏–Ω–∏—é
    mean_val = np.nanmean(data)
    ax.axvline(mean_val, color='red', linestyle='--', 
               label=f'Mean: {mean_val:.2f}')
    
    ax.legend()
    if title:
        ax.set_title(title)
    
    return ax
```

### 3.6 –≠–∫—Å–ø–æ—Ä—Ç –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–π

```python
def save_publication_figure(fig, filename, formats=['pdf', 'png', 'svg']):
    """
    –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∏–≥—É—Ä—É –≤–æ –≤—Å–µ—Ö –Ω—É–∂–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö.
    """
    for fmt in formats:
        fig.savefig(
            f"{filename}.{fmt}",
            dpi=300,
            bbox_inches='tight',
            facecolor='white',
            edgecolor='none'
        )
```

---

## 4. SciPy + Statsmodels ‚Äî –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ {#4-statistics}

### 4.1 –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã (scipy.stats)

```python
from scipy import stats

# T-—Ç–µ—Å—Ç—ã
t_stat, p_value = stats.ttest_ind(group1, group2)  # Independent
t_stat, p_value = stats.ttest_rel(group1, group2)  # Paired
t_stat, p_value = stats.ttest_1samp(data, popmean=0)  # One-sample

# Welch's t-test (–Ω–µ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç —Ä–∞–≤–Ω—ã–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏)
t_stat, p_value = stats.ttest_ind(group1, group2, equal_var=False)

# –ù–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ
u_stat, p_value = stats.mannwhitneyu(group1, group2, alternative='two-sided')
w_stat, p_value = stats.wilcoxon(group1, group2)  # Paired

# ANOVA
f_stat, p_value = stats.f_oneway(group1, group2, group3)

# Kruskal-Wallis
h_stat, p_value = stats.kruskal(group1, group2, group3)

# –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
r, p_value = stats.pearsonr(x, y)
rho, p_value = stats.spearmanr(x, y)
tau, p_value = stats.kendalltau(x, y)

# –ù–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å
w_stat, p_value = stats.shapiro(data)  # n < 5000
stat, p_value = stats.normaltest(data)  # D'Agostino-Pearson

# –û–¥–Ω–æ—Ä–æ–¥–Ω–æ—Å—Ç—å –¥–∏—Å–ø–µ—Ä—Å–∏–π
w_stat, p_value = stats.levene(group1, group2)
w_stat, p_value = stats.bartlett(group1, group2)  # –ï—Å–ª–∏ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ

# Chi-square
chi2, p_value, dof, expected = stats.chi2_contingency(table)
```

### 4.2 –†–µ–≥—Ä–µ—Å—Å–∏–∏ (statsmodels)

```python
import statsmodels.api as sm
import statsmodels.formula.api as smf

# –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å —Ñ–æ—Ä–º—É–ª–æ–π
model = smf.ols('outcome ~ treatment + age + C(sex)', data=df).fit()
print(model.summary())

# –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
model = smf.logit('event ~ treatment + age', data=df).fit()
odds_ratios = np.exp(model.params)

# Mixed Effects (LMM)
model = smf.mixedlm('outcome ~ time + treatment', 
                     data=df, 
                     groups=df['subject_id'],
                     re_formula='~time').fit()
```

### 4.3 Effect Sizes

| –¢–µ—Å—Ç | Effect Size | –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è |
|------|-------------|---------------|
| t-test | Cohen's d | 0.2 –º–∞–ª—ã–π, 0.5 —Å—Ä–µ–¥–Ω–∏–π, 0.8 –±–æ–ª—å—à–æ–π |
| ANOVA | Œ∑¬≤ (eta-squared) | 0.01 –º–∞–ª—ã–π, 0.06 —Å—Ä–µ–¥–Ω–∏–π, 0.14 –±–æ–ª—å—à–æ–π |
| ANOVA | œâ¬≤ (omega-squared) | –ú–µ–Ω–µ–µ —Å–º–µ—â—ë–Ω–Ω—ã–π —á–µ–º Œ∑¬≤ |
| Correlation | r | 0.1 —Å–ª–∞–±—ã–π, 0.3 —Å—Ä–µ–¥–Ω–∏–π, 0.5 —Å–∏–ª—å–Ω—ã–π |
| Chi-square | Cram√©r's V | –ó–∞–≤–∏—Å–∏—Ç –æ—Ç df |
| Odds | OR (Odds Ratio) | 1 = –Ω–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∞ |

```python
def cohens_d(group1, group2):
    """Cohen's d —Å pooled SD."""
    n1, n2 = len(group1), len(group2)
    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)
    pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))
    return (np.mean(group1) - np.mean(group2)) / pooled_std

def eta_squared(f_stat, df_between, df_within):
    """Eta-squared –¥–ª—è ANOVA."""
    return (f_stat * df_between) / (f_stat * df_between + df_within)
```

---

## 5. Scikit-learn ‚Äî ML –∏ –∏–º–ø—É—Ç–∞—Ü–∏—è {#5-machine-learning}

### 5.1 MICE –ò–º–ø—É—Ç–∞—Ü–∏—è

```python
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, SimpleImputer

# MICE ‚Äî –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –∏–º–ø—É—Ç–∞—Ü–∏—è
imputer = IterativeImputer(
    max_iter=10,
    random_state=42,
    sample_posterior=True  # –î–ª—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏
)
df_imputed = pd.DataFrame(
    imputer.fit_transform(df[numeric_cols]),
    columns=numeric_cols
)

# –ü—Ä–æ—Å—Ç–∞—è –∏–º–ø—É—Ç–∞—Ü–∏—è
imputer = SimpleImputer(strategy='median')  # –∏–ª–∏ 'mean', 'most_frequent'
```

### 5.2 –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

# Z-score (–¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö)
scaler = StandardScaler()

# Min-Max (–¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞)
scaler = MinMaxScaler()

# Robust (—É—Å—Ç–æ–π—á–∏–≤—ã–π –∫ –≤—ã–±—Ä–æ—Å–∞–º)
scaler = RobustScaler()

df[cols] = scaler.fit_transform(df[cols])
```

---

## 6. Pingouin ‚Äî –ë–∏–æ–º–µ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ {#6-pingouin}

**Pingouin** ‚Äî –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å —É–¥–æ–±–Ω—ã–º API –∏ –≥–æ—Ç–æ–≤—ã–º–∏ effect sizes.

```python
import pingouin as pg

# T-test —Å effect size
result = pg.ttest(group1, group2, correction='auto')  # auto-Welch
# –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: T, dof, p, d (Cohen's d), CI, power, BF10

# ANOVA —Å effect sizes
aov = pg.anova(data=df, dv='outcome', between='group')
# –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: Source, SS, DF, MS, F, p-unc, np2 (partial eta¬≤)

# Repeated measures ANOVA
rm_aov = pg.rm_anova(data=df, dv='outcome', within='time', subject='subject')

# Mixed ANOVA
mix_aov = pg.mixed_anova(data=df, dv='outcome', 
                          within='time', between='group', subject='subject')

# –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å CI
result = pg.corr(x, y, method='pearson')
# –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: n, r, CI95%, p, BF10, power

# Post-hoc —Ç–µ—Å—Ç—ã
posthoc = pg.pairwise_tukey(data=df, dv='outcome', between='group')
posthoc = pg.pairwise_tests(data=df, dv='outcome', between='group', 
                             padjust='bonf')  # bonferroni correction

# Power analysis
power = pg.power_ttest(d=0.5, n=None, power=0.8, alpha=0.05)
# –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π n
```

---

## üìã –ß–µ–∫–ª–∏—Å—Ç –¥–ª—è AI-–∞–≥–µ–Ω—Ç–æ–≤

### –ü—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–∞–Ω–Ω—ã–º–∏

- [ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Parquet –≤–º–µ—Å—Ç–æ CSV
- [ ] –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å dtypes (int64‚Üíint32, category)
- [ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å method chaining
- [ ] –ò–∑–±–µ–≥–∞—Ç—å —Ü–∏–∫–ª–æ–≤ ‚Äî —Ç–æ–ª—å–∫–æ vectorization

### –ü—Ä–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ

- [ ] –í—Å–µ–≥–¥–∞ —Å—á–∏—Ç–∞—Ç—å effect size
- [ ] –í—Å–µ–≥–¥–∞ –¥–∞–≤–∞—Ç—å 95% CI
- [ ] –ü—Ä–æ–≤–µ—Ä—è—Ç—å –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤ (–Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å, –≥–æ–º–æ–≥–µ–Ω–Ω–æ—Å—Ç—å)
- [ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å pingouin –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞

### –ü—Ä–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏

- [ ] 300 DPI –º–∏–Ω–∏–º—É–º
- [ ] Colorblind-safe –ø–∞–ª–∏—Ç—Ä—ã
- [ ] PDF/SVG –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–π
- [ ] –£–±–∏—Ä–∞—Ç—å –≤–µ—Ä—Ö–Ω—é—é –∏ –ø—Ä–∞–≤—É—é –æ—Å–∏

### –ü—Ä–∏ –≤—ã–≤–æ–¥–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

- [ ] APA-—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
- [ ] –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è effect size
- [ ] AI-–∑–∞–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏

---

*–í–µ—Ä—Å–∏—è: 1.0*  
*–ò—Å—Ç–æ—á–Ω–∏–∫–∏: Python Data Science Handbook, SciPy docs, APA 7th*
